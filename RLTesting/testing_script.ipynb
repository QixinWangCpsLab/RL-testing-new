{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject Bugs & Run Experiment\n",
    "\n",
    "![Inject Bugs & Run Experiment](./pic/RL_Testing.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozenlake Testbed Experiment\n",
    "\n",
    "- Find log files in RLTestig/logs/frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "Obtaining file:///Users/zhangshiyu/Documents/GitHub/stable-baselines3-testing-new\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from stable_baselines3==2.3.0a3) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.0a3) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3==2.3.0a3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3==2.3.0a3) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3==2.3.0a3) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3==2.3.0a3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3==2.3.0a3) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from torch>=1.13->stable_baselines3==2.3.0a3) (2024.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from matplotlib->stable_baselines3==2.3.0a3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from pandas->stable_baselines3==2.3.0a3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from pandas->stable_baselines3==2.3.0a3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3==2.3.0a3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable_baselines3==2.3.0a3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/zhangshiyu/opt/anaconda3/envs/stable-baselines3-testing-new/lib/python3.10/site-packages (from sympy->torch>=1.13->stable_baselines3==2.3.0a3) (1.3.0)\n",
      "Building wheels for collected packages: stable_baselines3\n",
      "  Building editable for stable_baselines3 (pyproject.toml): started\n",
      "  Building editable for stable_baselines3 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for stable_baselines3: filename=stable_baselines3-2.3.0a3-0.editable-py3-none-any.whl size=6132 sha256=f6f916c6225f94a135f21ba374c274b9db3e351dd5eb8d60b25b0410c74f9aca\n",
      "  Stored in directory: /private/var/folders/37/mwcx5pmx1k7c6s8w_rj88r080000gq/T/pip-ephem-wheel-cache-hd7v3hpq/wheels/d7/cc/c8/9f1179276555412526deb369fe7c4d1459447148a97140ce85\n",
      "Successfully built stable_baselines3\n",
      "Installing collected packages: stable_baselines3\n",
      "  Attempting uninstall: stable_baselines3\n",
      "    Found existing installation: stable_baselines3 2.3.0a3\n",
      "    Uninstalling stable_baselines3-2.3.0a3:\n",
      "      Successfully uninstalled stable_baselines3-2.3.0a3\n",
      "Successfully installed stable_baselines3-2.3.0a3\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n"
     ]
    }
   ],
   "source": [
    "import frozenlake.testing_SB3_Frozenlake as Frozenlake_Experiment\n",
    "\n",
    "# initialize bug_version_list\n",
    "bug_version_list = [\n",
    "    # [],\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    # [6],\n",
    "    # [7],\n",
    "    # [8],\n",
    "    # [9],\n",
    "    # [10],\n",
    "]\n",
    "\n",
    "Frozenlake_Experiment.main(bug_version_list=bug_version_list, rounds=25, epochs=300, model_type='dqn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mountaincar Testbed (continous) Experiment\n",
    "\n",
    "- Find log files in RLTestig/logs/mountaincar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountaincar.testing_SB3_Mountaincar as Mountaincar_Experiment\n",
    "\n",
    "bug_version_list = [\n",
    "    [],\n",
    "    # [5],\n",
    "    # [6],\n",
    "    # [7],\n",
    "    # [8],\n",
    "    # [10],\n",
    "    # [11],\n",
    "    # [12],\n",
    "    # [13],\n",
    "    # [14],\n",
    "    # [15]\n",
    "]\n",
    "\n",
    "Mountaincar_Experiment.main(bug_version_list=bug_version_list, rounds=10, epochs=300, model_type='ppo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import log_parser\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import config_parser\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "\n",
    "root_dir = config_parser.parserConfig()['root_dir']\n",
    "\n",
    "\n",
    "def linelar_regression(data, title = 'Data Trend Analysis', show=False):\n",
    "\n",
    "    # 将数据转换成Pandas Series对象\n",
    "    series = pd.Series(data)\n",
    "\n",
    "    # 计算简单移动平均(SMA)和指数移动平均(EMA)\n",
    "    sma = series.rolling(window=3).mean()\n",
    "    ema = series.ewm(span=3, adjust=False).mean()\n",
    "\n",
    "    # 使用线性回归判断趋势\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(range(len(data)), data)\n",
    "    \n",
    "    if show:\n",
    "        # 绘制数据和趋势线\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data, label='Original Data', marker='o')\n",
    "        plt.plot(range(len(data)), intercept + slope * np.asarray(range(len(data))), 'r', label=f'Trend Line: slope={slope:.2f}')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    # 输出线性回归结果\n",
    "    # print(f\"Slope of trend line: {slope:.2f}\")\n",
    "\n",
    "    # 如果斜率显著小于0，我们可以认为不存在上升趋势\n",
    "    if slope < 0:\n",
    "        return 'false'\n",
    "    else:\n",
    "        return 'true'\n",
    "    \n",
    "\n",
    "def test_true_proportion(num_samples, num_true, sig_level=0.05, expected_prob=0.95):\n",
    "    \"\"\"\n",
    "    对观察到的 'true' 样本数进行二项检验。\n",
    "\n",
    "    :param num_samples: 样本总数\n",
    "    :param num_true: 观察到的 'true' 样本数\n",
    "    :param sig_level: 显著性水平（默认为 0.05）\n",
    "    :param expected_prob: 预期成功概率（默认为 0.5）\n",
    "    :return: p 值和是否显著\n",
    "    \"\"\"\n",
    "    # 计算二项检验的 p 值\n",
    "    p_value = binomtest(k=num_true, n=num_samples, p=expected_prob, alternative='greater')\n",
    "\n",
    "    # 判断是否显著\n",
    "    is_significant = p_value < sig_level\n",
    "\n",
    "    return p_value, is_significant\n",
    "\n",
    "\n",
    "def bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=False):\n",
    "    # 使用 Path.rglob() 获取所有子文件\n",
    "    # '**/*' 表示匹配所有的文件和文件夹, 如果只想匹配文件, 使用 '**/*.*'\n",
    "    bug_free_log_list = [file for file in bug_free_log_path.rglob('*') if file.is_file()]\n",
    "    print(bug_free_log_list)\n",
    "\n",
    "    result = []\n",
    "    for path in bug_free_log_list:\n",
    "        # print(log_parser.parse_log_file(path))\n",
    "        data = log_parser.parse_log_file_fuzzy(path)\n",
    "        if len(data)> 0:\n",
    "            temp = linelar_regression(data, title=path, show=show_fig)\n",
    "        result.append(temp)\n",
    "\n",
    "    p_value = binomtest(result.count('true'), len(result), alternative='greater')\n",
    "    print(p_value)\n",
    "\n",
    "    \n",
    "    \n",
    "def bin_test_mountaincar(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Mountaincar', 'sac', '[]')), show_fig=False):\n",
    "    # 使用 Path.rglob() 获取所有子文件\n",
    "    # '**/*' 表示匹配所有的文件和文件夹, 如果只想匹配文件, 使用 '**/*.*'\n",
    "    bug_free_log_list = [file for file in bug_free_log_path.rglob('*') if file.is_file()]\n",
    "\n",
    "    print(bug_free_log_list)\n",
    "\n",
    "    # accuracy_list = []\n",
    "    # 打印所有文件路径\n",
    "    result = []\n",
    "    for path in bug_free_log_list:\n",
    "        data = log_parser.parse_mountaincar_log_file(path)\n",
    "        if len(data) > 0:\n",
    "            temp = linelar_regression(data, title=path, show=show_fig)\n",
    "        result.append(temp)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    在进行二项检验时，零假设（null hypothesis）通常是观察到的成功次数与随机变化相一致，即成功的概率等于 expected_prob。如果实际观察到的成功次数显著高于（或低于）这个期望成功概率，那么您可能会得到一个很小的 p 值，从而拒绝零假设。\n",
    "    '''\n",
    "    p_value = binomtest(result.count('true'), len(result), alternative='greater')\n",
    "    print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=False)\n",
    "# bin_test_mountaincar(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Mountaincar', 'a2c', '[]')), show_fig=False)\n",
    "bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB3Testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
