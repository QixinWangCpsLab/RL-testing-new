{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject Bugs & Run Experiment\n",
    "\n",
    "![Inject Bugs & Run Experiment](./pic/RL_Testing.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozenlake Testbed Experiment\n",
    "\n",
    "- Find log files in RLTestig/logs/frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0----\n",
      "creating new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iansy\\.conda\\envs\\SB3Testing_new\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iansy\\.conda\\envs\\SB3Testing_new\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.desc to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.desc` for environment variables or `env.get_wrapper_attr('desc')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iansy\\.conda\\envs\\SB3Testing_new\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_state_action_pairs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_state_action_pairs` for environment variables or `env.get_wrapper_attr('get_state_action_pairs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import warnings\n",
      "from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch.nn import functional as F\n",
      "\n",
      "from stable_baselines3.common.buffers import ReplayBuffer\n",
      "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import get_linear_fn, get_parameters_by_name, polyak_update\n",
      "from stable_baselines3.dqn.policies import CnnPolicy, DQNPolicy, MlpPolicy, MultiInputPolicy, QNetwork\n",
      "\n",
      "SelfDQN = TypeVar(\"SelfDQN\", bound=\"DQN\")\n",
      "\n",
      "\n",
      "class DQN(OffPolicyAlgorithm):\n",
      "    \"\"\"\n",
      "    Deep Q-Network (DQN)\n",
      "\n",
      "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
      "    Default hyperparameters are taken from the Nature paper,\n",
      "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param buffer_size: size of the replay buffer\n",
      "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
      "    :param batch_size: Minibatch size for each gradient update\n",
      "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
      "    :param gamma: the discount factor\n",
      "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
      "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
      "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
      "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
      "        during the rollout.\n",
      "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
      "        If ``None``, it will be automatically selected.\n",
      "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
      "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
      "        at a cost of more complexity.\n",
      "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
      "    :param target_update_interval: update the target network every ``target_update_interval``\n",
      "        environment steps.\n",
      "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
      "    :param exploration_initial_eps: initial value of random action probability\n",
      "    :param exploration_final_eps: final value of random action probability\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    \"\"\"\n",
      "\n",
      "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
      "        \"MlpPolicy\": MlpPolicy,\n",
      "        \"CnnPolicy\": CnnPolicy,\n",
      "        \"MultiInputPolicy\": MultiInputPolicy,\n",
      "    }\n",
      "    # Linear schedule will be defined in `_setup_model()`\n",
      "    exploration_schedule: Schedule\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "    policy: DQNPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[DQNPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule] = 1e-4,\n",
      "        buffer_size: int = 1_000_000,  # 1e6\n",
      "        learning_starts: int = 100,\n",
      "        batch_size: int = 32,\n",
      "        tau: float = 1.0,\n",
      "        gamma: float = 0.99,\n",
      "        train_freq: Union[int, Tuple[int, str]] = 4,\n",
      "        gradient_steps: int = 1,\n",
      "        replay_buffer_class: Optional[Type[ReplayBuffer]] = None,\n",
      "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        optimize_memory_usage: bool = False,\n",
      "        target_update_interval: int = 10000,\n",
      "        exploration_fraction: float = 0.1,\n",
      "        exploration_initial_eps: float = 1.0,\n",
      "        exploration_final_eps: float = 0.05,\n",
      "        max_grad_norm: float = 10,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            policy,\n",
      "            env,\n",
      "            learning_rate,\n",
      "            buffer_size,\n",
      "            learning_starts,\n",
      "            batch_size,\n",
      "            tau,\n",
      "            gamma,\n",
      "            train_freq,\n",
      "            gradient_steps,\n",
      "            action_noise=None,  # No action noise\n",
      "            replay_buffer_class=replay_buffer_class,\n",
      "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            seed=seed,\n",
      "            sde_support=False,\n",
      "            optimize_memory_usage=optimize_memory_usage,\n",
      "            supported_action_spaces=(spaces.Discrete,),\n",
      "            support_multi_env=True,\n",
      "        )\n",
      "\n",
      "        self.exploration_initial_eps = exploration_initial_eps\n",
      "        self.exploration_final_eps = exploration_final_eps\n",
      "        self.exploration_fraction = exploration_fraction\n",
      "        self.target_update_interval = target_update_interval\n",
      "        # For updating the target network with multiple envs:\n",
      "        self._n_calls = 0\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        # \"epsilon\" for the epsilon-greedy exploration\n",
      "        self.exploration_rate = 0.0\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        super()._setup_model()\n",
      "        self._create_aliases()\n",
      "        # Copy running stats, see GH issue #996\n",
      "        self.batch_norm_stats = get_parameters_by_name(self.q_net, [\"running_\"])\n",
      "        self.batch_norm_stats_target = get_parameters_by_name(self.q_net_target, [\"running_\"])\n",
      "        self.exploration_schedule = get_linear_fn(\n",
      "            self.exploration_initial_eps,\n",
      "            self.exploration_final_eps, #\n",
      "            self.exploration_fraction,\n",
      "        )\n",
      "\n",
      "        if self.n_envs > 1:\n",
      "            if self.n_envs > self.target_update_interval:\n",
      "                warnings.warn(\n",
      "                    \"The number of environments used is greater than the target network \"\n",
      "                    f\"update interval ({self.n_envs} > {self.target_update_interval}), \"\n",
      "                    \"therefore the target network will be updated after each call to env.step() \"\n",
      "                    f\"which corresponds to {self.n_envs} steps.\"\n",
      "                )\n",
      "\n",
      "    def _create_aliases(self) -> None:\n",
      "        self.q_net = self.policy.q_net\n",
      "        self.q_net_target = self.policy.q_net_target\n",
      "\n",
      "    def _on_step(self) -> None:\n",
      "        \"\"\"\n",
      "        Update the exploration rate and target network if needed.\n",
      "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
      "        \"\"\"\n",
      "        self._n_calls += 1\n",
      "        # Account for multiple environments\n",
      "        # each call to step() corresponds to n_envs transitions\n",
      "        if self._n_calls % max(self.target_update_interval // self.n_envs, 1) == 0:\n",
      "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
      "            # Copy running stats, see GH issue #996\n",
      "            polyak_update(self.batch_norm_stats, self.batch_norm_stats_target, 1.0)\n",
      "\n",
      "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
      "        self.logger.record(\"rollout/exploration_rate\", self.exploration_rate)\n",
      "\n",
      "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
      "        # Switch to train mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(True)\n",
      "        # Update learning rate according to schedule\n",
      "        self._update_learning_rate(self.policy.optimizer)\n",
      "\n",
      "        losses = []\n",
      "        for _ in range(gradient_steps):\n",
      "            # Sample replay buffer\n",
      "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Compute the next Q-values using the target network\n",
      "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
      "                # Follow greedy policy: use the one with the highest value\n",
      "                next_q_values, _ = next_q_values.max(dim=1)\n",
      "                # Avoid potential broadcast issue\n",
      "                next_q_values = next_q_values.reshape(-1, 1)\n",
      "                # 1-step TD target\n",
      "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
      "\n",
      "            # Get current Q-values estimates\n",
      "            current_q_values = self.q_net(replay_data.observations)\n",
      "\n",
      "            # Retrieve the q-values for the actions from the replay buffer\n",
      "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
      "\n",
      "            # Compute Huber loss (less sensitive to outliers)\n",
      "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
      "            losses.append(loss.item())\n",
      "\n",
      "            # Optimize the policy\n",
      "            self.policy.optimizer.zero_grad()\n",
      "            loss.backward()\n",
      "            # Clip gradient norm\n",
      "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
      "            self.policy.optimizer.step()\n",
      "\n",
      "        # Increase update counter\n",
      "        self._n_updates += gradient_steps\n",
      "\n",
      "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
      "        self.logger.record(\"train/loss\", np.mean(losses))\n",
      "\n",
      "    def predict(\n",
      "        self,\n",
      "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
      "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
      "        episode_start: Optional[np.ndarray] = None,\n",
      "        deterministic: bool = False,\n",
      "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
      "        \"\"\"\n",
      "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
      "\n",
      "        :param observation: the input observation\n",
      "        :param state: The last states (can be None, used in recurrent policies)\n",
      "        :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      "        :param deterministic: Whether or not to return deterministic actions.\n",
      "        :return: the model's action and the next state\n",
      "            (used in recurrent policies)\n",
      "        \"\"\"\n",
      "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
      "            if self.policy.is_vectorized_observation(observation):\n",
      "                if isinstance(observation, dict):\n",
      "                    n_batch = observation[next(iter(observation.keys()))].shape[0]\n",
      "                else:\n",
      "                    n_batch = observation.shape[0]\n",
      "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
      "            else:\n",
      "                action = np.array(self.action_space.sample())\n",
      "        else:\n",
      "            action, state = self.policy.predict(observation, state, episode_start, deterministic)\n",
      "        return action, state\n",
      "\n",
      "    def learn(\n",
      "        self: SelfDQN,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 4,\n",
      "        tb_log_name: str = \"DQN\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfDQN:\n",
      "        return super().learn(\n",
      "            total_timesteps=total_timesteps,\n",
      "            callback=callback,\n",
      "            log_interval=log_interval,\n",
      "            tb_log_name=tb_log_name,\n",
      "            reset_num_timesteps=reset_num_timesteps,\n",
      "            progress_bar=progress_bar,\n",
      "        )\n",
      "\n",
      "    def _excluded_save_params(self) -> List[str]:\n",
      "        return [*super()._excluded_save_params(), \"q_net\", \"q_net_target\"]\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import sys\n",
      "import time\n",
      "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "\n",
      "from stable_baselines3.common.base_class import BaseAlgorithm\n",
      "from stable_baselines3.common.buffers import DictRolloutBuffer, RolloutBuffer\n",
      "from stable_baselines3.common.callbacks import BaseCallback\n",
      "from stable_baselines3.common.policies import ActorCriticPolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import obs_as_tensor, safe_mean\n",
      "from stable_baselines3.common.vec_env import VecEnv\n",
      "\n",
      "SelfOnPolicyAlgorithm = TypeVar(\"SelfOnPolicyAlgorithm\", bound=\"OnPolicyAlgorithm\")\n",
      "\n",
      "\n",
      "class OnPolicyAlgorithm(BaseAlgorithm):\n",
      "    \"\"\"\n",
      "    The base for On-Policy algorithms (ex: A2C/PPO).\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param n_steps: The number of steps to run for each environment per update\n",
      "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
      "    :param gamma: Discount factor\n",
      "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
      "        Equivalent to classic advantage when set to 1.\n",
      "    :param ent_coef: Entropy coefficient for the loss calculation\n",
      "    :param vf_coef: Value function coefficient for the loss calculation\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      "        instead of action noise exploration (default: False)\n",
      "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      "        Default: -1 (only sample at the beginning of the rollout)\n",
      "    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
      "    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation.\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
      "        or not in a Monitor wrapper.\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
      "    \"\"\"\n",
      "\n",
      "    rollout_buffer: RolloutBuffer\n",
      "    policy: ActorCriticPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[ActorCriticPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule],\n",
      "        n_steps: int,\n",
      "        gamma: float,\n",
      "        gae_lambda: float,\n",
      "        ent_coef: float,\n",
      "        vf_coef: float,\n",
      "        max_grad_norm: float,\n",
      "        use_sde: bool,\n",
      "        sde_sample_freq: int,\n",
      "        rollout_buffer_class: Optional[Type[RolloutBuffer]] = None,\n",
      "        rollout_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        monitor_wrapper: bool = True,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "        supported_action_spaces: Optional[Tuple[Type[spaces.Space], ...]] = None,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            policy=policy,\n",
      "            env=env,\n",
      "            learning_rate=learning_rate,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            use_sde=use_sde,\n",
      "            sde_sample_freq=sde_sample_freq,\n",
      "            support_multi_env=True,\n",
      "            seed=seed,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            supported_action_spaces=supported_action_spaces,\n",
      "        )\n",
      "\n",
      "        self.n_steps = n_steps\n",
      "        self.gamma = gamma\n",
      "        self.gae_lambda = gae_lambda\n",
      "        self.ent_coef = ent_coef\n",
      "        self.vf_coef = vf_coef\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        self.rollout_buffer_class = rollout_buffer_class\n",
      "        self.rollout_buffer_kwargs = rollout_buffer_kwargs or {}\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        self._setup_lr_schedule()\n",
      "        self.set_random_seed(self.seed)\n",
      "\n",
      "        if self.rollout_buffer_class is None:\n",
      "            if isinstance(self.observation_space, spaces.Dict):\n",
      "                self.rollout_buffer_class = DictRolloutBuffer\n",
      "            else:\n",
      "                self.rollout_buffer_class = RolloutBuffer\n",
      "\n",
      "        self.rollout_buffer = self.rollout_buffer_class(\n",
      "            self.n_steps,\n",
      "            self.observation_space,  # type: ignore[arg-type]\n",
      "            self.action_space,\n",
      "            device=self.device,\n",
      "            gamma=self.gamma,\n",
      "            gae_lambda=self.gae_lambda,\n",
      "            n_envs=self.n_envs,\n",
      "            **self.rollout_buffer_kwargs,\n",
      "        )\n",
      "        self.policy = self.policy_class(  # type: ignore[assignment]\n",
      "            self.observation_space, self.action_space, self.lr_schedule, use_sde=self.use_sde, **self.policy_kwargs\n",
      "        )\n",
      "        self.policy = self.policy.to(self.device)\n",
      "\n",
      "    def collect_rollouts(\n",
      "        self,\n",
      "        env: VecEnv,\n",
      "        callback: BaseCallback,\n",
      "        rollout_buffer: RolloutBuffer,\n",
      "        n_rollout_steps: int,\n",
      "    ) -> bool:\n",
      "        \"\"\"\n",
      "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
      "        The term rollout here refers to the model-free notion and should not\n",
      "        be used with the concept of rollout used in model-based RL or planning.\n",
      "\n",
      "        :param env: The training environment\n",
      "        :param callback: Callback that will be called at each step\n",
      "            (and at the beginning and end of the rollout)\n",
      "        :param rollout_buffer: Buffer to fill with rollouts\n",
      "        :param n_rollout_steps: Number of experiences to collect per environment\n",
      "        :return: True if function returned with at least `n_rollout_steps`\n",
      "            collected, False if callback terminated rollout prematurely.\n",
      "        \"\"\"\n",
      "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
      "        # Switch to eval mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(False)\n",
      "\n",
      "        n_steps = 0\n",
      "        rollout_buffer.reset()\n",
      "        # Sample new weights for the state dependent exploration\n",
      "        if self.use_sde:\n",
      "            self.policy.reset_noise(env.num_envs)\n",
      "\n",
      "        callback.on_rollout_start()\n",
      "\n",
      "        while n_steps < n_rollout_steps:\n",
      "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
      "                # Sample a new noise matrix\n",
      "                self.policy.reset_noise(env.num_envs)\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Convert to pytorch tensor or to TensorDict\n",
      "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
      "                actions, values, log_probs = self.policy(obs_tensor)\n",
      "            actions = actions.cpu().numpy()\n",
      "\n",
      "            # Rescale and perform action\n",
      "            clipped_actions = actions\n",
      "\n",
      "            if isinstance(self.action_space, spaces.Box):\n",
      "                if self.policy.squash_output:\n",
      "                    # Unscale the actions to match env bounds\n",
      "                    # if they were previously squashed (scaled in [-1, 1])\n",
      "                    clipped_actions = self.policy.unscale_action(clipped_actions)\n",
      "                else:\n",
      "                    # Otherwise, clip the actions to avoid out of bound error\n",
      "                    # as we are sampling from an unbounded Gaussian distribution\n",
      "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
      "\n",
      "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
      "\n",
      "            self.num_timesteps += env.num_envs\n",
      "\n",
      "            # Give access to local variables\n",
      "            callback.update_locals(locals())\n",
      "            if not callback.on_step():\n",
      "                return False\n",
      "\n",
      "            self._update_info_buffer(infos)\n",
      "            n_steps += 1\n",
      "\n",
      "            if isinstance(self.action_space, spaces.Discrete):\n",
      "                # Reshape in case of discrete action\n",
      "                actions = actions.reshape(-1, 1)\n",
      "\n",
      "            # Handle timeout by bootstraping with value function\n",
      "            # see GitHub issue #633\n",
      "            for idx, done in enumerate(dones):\n",
      "                if (\n",
      "                    done\n",
      "                    and infos[idx].get(\"terminal_observation\") is not None\n",
      "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
      "                ):\n",
      "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
      "                    with th.no_grad():\n",
      "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
      "                    rewards[idx] += self.gamma * terminal_value\n",
      "\n",
      "            rollout_buffer.add(\n",
      "                self._last_obs,  # type: ignore[arg-type]\n",
      "                actions,\n",
      "                rewards,\n",
      "                self._last_episode_starts,  # type: ignore[arg-type]\n",
      "                values,\n",
      "                log_probs,\n",
      "            )\n",
      "            self._last_obs = new_obs  # type: ignore[assignment]\n",
      "            self._last_episode_starts = dones\n",
      "\n",
      "        with th.no_grad(): #10th bug: 1\n",
      "            # Compute value for the last timestep\n",
      "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))  # type: ignore[arg-type]\n",
      "\n",
      "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
      "\n",
      "        callback.update_locals(locals())\n",
      "\n",
      "        callback.on_rollout_end()\n",
      "\n",
      "        return True\n",
      "\n",
      "    def train(self) -> None:\n",
      "        \"\"\"\n",
      "        Consume current rollout data and update policy parameters.\n",
      "        Implemented by individual algorithms.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def learn(\n",
      "        self: SelfOnPolicyAlgorithm,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 1,\n",
      "        tb_log_name: str = \"OnPolicyAlgorithm\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfOnPolicyAlgorithm:\n",
      "        iteration = 0\n",
      "\n",
      "        total_timesteps, callback = self._setup_learn(\n",
      "            total_timesteps,\n",
      "            callback,\n",
      "            reset_num_timesteps,\n",
      "            tb_log_name,\n",
      "            progress_bar,\n",
      "        )\n",
      "\n",
      "        callback.on_training_start(locals(), globals())\n",
      "\n",
      "        assert self.env is not None\n",
      "\n",
      "        while self.num_timesteps < total_timesteps:\n",
      "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "\n",
      "            if not continue_training:\n",
      "                break\n",
      "\n",
      "            iteration += 1\n",
      "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
      "\n",
      "            # Display training infos\n",
      "            if log_interval is not None and iteration % log_interval == 0:\n",
      "                assert self.ep_info_buffer is not None\n",
      "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
      "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
      "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
      "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
      "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
      "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
      "                self.logger.record(\"time/fps\", fps)\n",
      "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
      "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
      "                self.logger.dump(step=self.num_timesteps)\n",
      "\n",
      "            self.train()\n",
      "\n",
      "        callback.on_training_end()\n",
      "\n",
      "        return self\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "from typing import Any, Dict, List, Optional, Type\n",
      "\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "from torch import nn\n",
      "\n",
      "from stable_baselines3.common.policies import BasePolicy\n",
      "from stable_baselines3.common.torch_layers import (\n",
      "    BaseFeaturesExtractor,\n",
      "    CombinedExtractor,\n",
      "    FlattenExtractor,\n",
      "    NatureCNN,\n",
      "    create_mlp,\n",
      ")\n",
      "from stable_baselines3.common.type_aliases import PyTorchObs, Schedule\n",
      "\n",
      "\n",
      "class QNetwork(BasePolicy):\n",
      "    \"\"\"\n",
      "    Action-Value (Q-Value) network for DQN\n",
      "\n",
      "    :param observation_space: Observation space\n",
      "    :param action_space: Action space\n",
      "    :param net_arch: The specification of the policy and value networks.\n",
      "    :param activation_fn: Activation function\n",
      "    :param normalize_images: Whether to normalize images or not,\n",
      "         dividing by 255.0 (True by default)\n",
      "    \"\"\"\n",
      "\n",
      "    action_space: spaces.Discrete\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        observation_space: spaces.Space,\n",
      "        action_space: spaces.Discrete,\n",
      "        features_extractor: BaseFeaturesExtractor,\n",
      "        features_dim: int,\n",
      "        net_arch: Optional[List[int]] = None,\n",
      "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
      "        normalize_images: bool = True,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            observation_space,\n",
      "            action_space,\n",
      "            features_extractor=features_extractor,\n",
      "            normalize_images=normalize_images,\n",
      "        )\n",
      "\n",
      "        if net_arch is None:\n",
      "            net_arch = [64, 64]\n",
      "\n",
      "        self.net_arch = net_arch\n",
      "        self.activation_fn = activation_fn\n",
      "        self.features_dim = features_dim\n",
      "        action_dim = int(self.action_space.n)  # number of actions\n",
      "        q_net = create_mlp(self.features_dim, action_dim, self.net_arch, self.activation_fn)\n",
      "        self.q_net = nn.Sequential(*q_net)\n",
      "\n",
      "    def forward(self, obs: PyTorchObs) -> th.Tensor:\n",
      "        \"\"\"\n",
      "        Predict the q-values.\n",
      "\n",
      "        :param obs: Observation\n",
      "        :return: The estimated Q-Value for each action.\n",
      "        \"\"\"\n",
      "        return self.q_net(self.extract_features(obs, self.features_extractor))\n",
      "\n",
      "    def _predict(self, observation: PyTorchObs, deterministic: bool = True) -> th.Tensor:\n",
      "        q_values = self(observation)\n",
      "        # Greedy action\n",
      "        action = q_values.argmax(dim=1).reshape(-1)\n",
      "        return action\n",
      "\n",
      "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
      "        data = super()._get_constructor_parameters()\n",
      "\n",
      "        data.update(\n",
      "            dict(\n",
      "                net_arch=self.net_arch,\n",
      "                features_dim=self.features_dim,\n",
      "                activation_fn=self.activation_fn,\n",
      "                features_extractor=self.features_extractor,\n",
      "            )\n",
      "        )\n",
      "        return data\n",
      "\n",
      "\n",
      "class DQNPolicy(BasePolicy):\n",
      "    \"\"\"\n",
      "    Policy class with Q-Value Net and target net for DQN\n",
      "\n",
      "    :param observation_space: Observation space\n",
      "    :param action_space: Action space\n",
      "    :param lr_schedule: Learning rate schedule (could be constant)\n",
      "    :param net_arch: The specification of the policy and value networks.\n",
      "    :param activation_fn: Activation function\n",
      "    :param features_extractor_class: Features extractor to use.\n",
      "    :param features_extractor_kwargs: Keyword arguments\n",
      "        to pass to the features extractor.\n",
      "    :param normalize_images: Whether to normalize images or not,\n",
      "         dividing by 255.0 (True by default)\n",
      "    :param optimizer_class: The optimizer to use,\n",
      "        ``th.optim.Adam`` by default\n",
      "    :param optimizer_kwargs: Additional keyword arguments,\n",
      "        excluding the learning rate, to pass to the optimizer\n",
      "    \"\"\"\n",
      "\n",
      "    q_net: QNetwork\n",
      "    q_net_target: QNetwork\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        observation_space: spaces.Space,\n",
      "        action_space: spaces.Discrete,\n",
      "        lr_schedule: Schedule,\n",
      "        net_arch: Optional[List[int]] = None,\n",
      "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
      "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
      "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        normalize_images: bool = True,\n",
      "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
      "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            observation_space,\n",
      "            action_space,\n",
      "            features_extractor_class,\n",
      "            features_extractor_kwargs,\n",
      "            optimizer_class=optimizer_class,\n",
      "            optimizer_kwargs=optimizer_kwargs,\n",
      "            normalize_images=normalize_images,\n",
      "        )\n",
      "\n",
      "        if net_arch is None:\n",
      "            if features_extractor_class == NatureCNN:\n",
      "                net_arch = []\n",
      "            else:\n",
      "                net_arch = [64, 64]\n",
      "\n",
      "        #9th bug: 1\n",
      "        #9th bug: 2 \n",
      "        self.net_arch = net_arch\n",
      "        self.activation_fn = activation_fn\n",
      "\n",
      "        self.net_args = {\n",
      "            \"observation_space\": self.observation_space,\n",
      "            \"action_space\": self.action_space,\n",
      "            #9th bug: 3\n",
      "            #9th bug: 4\n",
      "            \"net_arch\": self.net_arch,\n",
      "            \"activation_fn\": self.activation_fn,\n",
      "            \"normalize_images\": normalize_images,\n",
      "        }\n",
      "\n",
      "        self._build(lr_schedule)\n",
      "\n",
      "    def _build(self, lr_schedule: Schedule) -> None:\n",
      "        \"\"\"\n",
      "        Create the network and the optimizer.\n",
      "\n",
      "        Put the target network into evaluation mode.\n",
      "\n",
      "        :param lr_schedule: Learning rate schedule\n",
      "            lr_schedule(1) is the initial learning rate\n",
      "        \"\"\"\n",
      "\n",
      "        self.q_net = self.make_q_net()\n",
      "        self.q_net_target = self.make_q_net()\n",
      "        self.q_net_target.load_state_dict(self.q_net.state_dict())\n",
      "        self.q_net_target.set_training_mode(False)\n",
      "\n",
      "        # Setup optimizer with initial learning rate\n",
      "        self.optimizer = self.optimizer_class(  # type: ignore[call-arg]\n",
      "            self.parameters(),\n",
      "            lr=lr_schedule(1),\n",
      "            **self.optimizer_kwargs,\n",
      "        )\n",
      "\n",
      "    def make_q_net(self) -> QNetwork:\n",
      "        # Make sure we always have separate networks for features extractors etc\n",
      "        net_args = self._update_features_extractor(self.net_args, features_extractor=None)\n",
      "        return QNetwork(**net_args).to(self.device)\n",
      "\n",
      "    def forward(self, obs: PyTorchObs, deterministic: bool = True) -> th.Tensor:\n",
      "        return self._predict(obs, deterministic=deterministic)\n",
      "\n",
      "    def _predict(self, obs: PyTorchObs, deterministic: bool = True) -> th.Tensor:\n",
      "        return self.q_net._predict(obs, deterministic=deterministic)\n",
      "\n",
      "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
      "        data = super()._get_constructor_parameters()\n",
      "\n",
      "        data.update(\n",
      "            dict(\n",
      "                net_arch=self.net_args[\"net_arch\"],\n",
      "                activation_fn=self.net_args[\"activation_fn\"],\n",
      "                lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n",
      "                optimizer_class=self.optimizer_class,\n",
      "                optimizer_kwargs=self.optimizer_kwargs,\n",
      "                features_extractor_class=self.features_extractor_class,\n",
      "                features_extractor_kwargs=self.features_extractor_kwargs,\n",
      "            )\n",
      "        )\n",
      "        return data\n",
      "\n",
      "    def set_training_mode(self, mode: bool) -> None:\n",
      "        \"\"\"\n",
      "        Put the policy in either training or evaluation mode.\n",
      "\n",
      "        This affects certain modules, such as batch normalisation and dropout.\n",
      "\n",
      "        :param mode: if true, set to training mode, else set to evaluation mode\n",
      "        \"\"\"\n",
      "        self.q_net.set_training_mode(mode)\n",
      "        self.training = mode\n",
      "\n",
      "\n",
      "MlpPolicy = DQNPolicy\n",
      "\n",
      "\n",
      "class CnnPolicy(DQNPolicy):\n",
      "    \"\"\"\n",
      "    Policy class for DQN when using images as input.\n",
      "\n",
      "    :param observation_space: Observation space\n",
      "    :param action_space: Action space\n",
      "    :param lr_schedule: Learning rate schedule (could be constant)\n",
      "    :param net_arch: The specification of the policy and value networks.\n",
      "    :param activation_fn: Activation function\n",
      "    :param features_extractor_class: Features extractor to use.\n",
      "    :param normalize_images: Whether to normalize images or not,\n",
      "         dividing by 255.0 (True by default)\n",
      "    :param optimizer_class: The optimizer to use,\n",
      "        ``th.optim.Adam`` by default\n",
      "    :param optimizer_kwargs: Additional keyword arguments,\n",
      "        excluding the learning rate, to pass to the optimizer\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        observation_space: spaces.Space,\n",
      "        action_space: spaces.Discrete,\n",
      "        lr_schedule: Schedule,\n",
      "        net_arch: Optional[List[int]] = None,\n",
      "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
      "        features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n",
      "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        normalize_images: bool = True,\n",
      "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
      "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            observation_space,\n",
      "            action_space,\n",
      "            lr_schedule,\n",
      "            net_arch,\n",
      "            activation_fn,\n",
      "            features_extractor_class,\n",
      "            features_extractor_kwargs,\n",
      "            normalize_images,\n",
      "            optimizer_class,\n",
      "            optimizer_kwargs,\n",
      "        )\n",
      "\n",
      "\n",
      "class MultiInputPolicy(DQNPolicy):\n",
      "    \"\"\"\n",
      "    Policy class for DQN when using dict observations as input.\n",
      "\n",
      "    :param observation_space: Observation space\n",
      "    :param action_space: Action space\n",
      "    :param lr_schedule: Learning rate schedule (could be constant)\n",
      "    :param net_arch: The specification of the policy and value networks.\n",
      "    :param activation_fn: Activation function\n",
      "    :param features_extractor_class: Features extractor to use.\n",
      "    :param normalize_images: Whether to normalize images or not,\n",
      "         dividing by 255.0 (True by default)\n",
      "    :param optimizer_class: The optimizer to use,\n",
      "        ``th.optim.Adam`` by default\n",
      "    :param optimizer_kwargs: Additional keyword arguments,\n",
      "        excluding the learning rate, to pass to the optimizer\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        observation_space: spaces.Dict,\n",
      "        action_space: spaces.Discrete,\n",
      "        lr_schedule: Schedule,\n",
      "        net_arch: Optional[List[int]] = None,\n",
      "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
      "        features_extractor_class: Type[BaseFeaturesExtractor] = CombinedExtractor,\n",
      "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        normalize_images: bool = True,\n",
      "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
      "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "    ) -> None:\n",
      "        super().__init__(\n",
      "            observation_space,\n",
      "            action_space,\n",
      "            lr_schedule,\n",
      "            net_arch,\n",
      "            activation_fn,\n",
      "            features_extractor_class,\n",
      "            features_extractor_kwargs,\n",
      "            normalize_images,\n",
      "            optimizer_class,\n",
      "            optimizer_kwargs,\n",
      "        )\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n",
      "import sys\n",
      "import time\n",
      "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch as th\n",
      "from gymnasium import spaces\n",
      "\n",
      "from stable_baselines3.common.base_class import BaseAlgorithm\n",
      "from stable_baselines3.common.buffers import DictRolloutBuffer, RolloutBuffer\n",
      "from stable_baselines3.common.callbacks import BaseCallback\n",
      "from stable_baselines3.common.policies import ActorCriticPolicy\n",
      "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
      "from stable_baselines3.common.utils import obs_as_tensor, safe_mean\n",
      "from stable_baselines3.common.vec_env import VecEnv\n",
      "\n",
      "SelfOnPolicyAlgorithm = TypeVar(\"SelfOnPolicyAlgorithm\", bound=\"OnPolicyAlgorithm\")\n",
      "\n",
      "\n",
      "class OnPolicyAlgorithm(BaseAlgorithm):\n",
      "    \"\"\"\n",
      "    The base for On-Policy algorithms (ex: A2C/PPO).\n",
      "\n",
      "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
      "    :param learning_rate: The learning rate, it can be a function\n",
      "        of the current progress remaining (from 1 to 0)\n",
      "    :param n_steps: The number of steps to run for each environment per update\n",
      "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
      "    :param gamma: Discount factor\n",
      "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
      "        Equivalent to classic advantage when set to 1.\n",
      "    :param ent_coef: Entropy coefficient for the loss calculation\n",
      "    :param vf_coef: Value function coefficient for the loss calculation\n",
      "    :param max_grad_norm: The maximum value for the gradient clipping\n",
      "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
      "        instead of action noise exploration (default: False)\n",
      "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
      "        Default: -1 (only sample at the beginning of the rollout)\n",
      "    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
      "    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation.\n",
      "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      "        the reported success rate, mean episode length, and mean reward over\n",
      "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
      "        or not in a Monitor wrapper.\n",
      "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
      "        debug messages\n",
      "    :param seed: Seed for the pseudo random generators\n",
      "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      "        Setting it to auto, the code will be run on the GPU if possible.\n",
      "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
      "    \"\"\"\n",
      "\n",
      "    rollout_buffer: RolloutBuffer\n",
      "    policy: ActorCriticPolicy\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        policy: Union[str, Type[ActorCriticPolicy]],\n",
      "        env: Union[GymEnv, str],\n",
      "        learning_rate: Union[float, Schedule],\n",
      "        n_steps: int,\n",
      "        gamma: float,\n",
      "        gae_lambda: float,\n",
      "        ent_coef: float,\n",
      "        vf_coef: float,\n",
      "        max_grad_norm: float,\n",
      "        use_sde: bool,\n",
      "        sde_sample_freq: int,\n",
      "        rollout_buffer_class: Optional[Type[RolloutBuffer]] = None,\n",
      "        rollout_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        stats_window_size: int = 100,\n",
      "        tensorboard_log: Optional[str] = None,\n",
      "        monitor_wrapper: bool = True,\n",
      "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        verbose: int = 0,\n",
      "        seed: Optional[int] = None,\n",
      "        device: Union[th.device, str] = \"auto\",\n",
      "        _init_setup_model: bool = True,\n",
      "        supported_action_spaces: Optional[Tuple[Type[spaces.Space], ...]] = None,\n",
      "    ):\n",
      "        super().__init__(\n",
      "            policy=policy,\n",
      "            env=env,\n",
      "            learning_rate=learning_rate,\n",
      "            policy_kwargs=policy_kwargs,\n",
      "            verbose=verbose,\n",
      "            device=device,\n",
      "            use_sde=use_sde,\n",
      "            sde_sample_freq=sde_sample_freq,\n",
      "            support_multi_env=True,\n",
      "            seed=seed,\n",
      "            stats_window_size=stats_window_size,\n",
      "            tensorboard_log=tensorboard_log,\n",
      "            supported_action_spaces=supported_action_spaces,\n",
      "        )\n",
      "\n",
      "        self.n_steps = n_steps\n",
      "        self.gamma = gamma\n",
      "        self.gae_lambda = gae_lambda\n",
      "        self.ent_coef = ent_coef\n",
      "        self.vf_coef = vf_coef\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        self.rollout_buffer_class = rollout_buffer_class\n",
      "        self.rollout_buffer_kwargs = rollout_buffer_kwargs or {}\n",
      "\n",
      "        if _init_setup_model:\n",
      "            self._setup_model()\n",
      "\n",
      "    def _setup_model(self) -> None:\n",
      "        self._setup_lr_schedule()\n",
      "        self.set_random_seed(self.seed)\n",
      "\n",
      "        if self.rollout_buffer_class is None:\n",
      "            if isinstance(self.observation_space, spaces.Dict):\n",
      "                self.rollout_buffer_class = DictRolloutBuffer\n",
      "            else:\n",
      "                self.rollout_buffer_class = RolloutBuffer\n",
      "\n",
      "        self.rollout_buffer = self.rollout_buffer_class(\n",
      "            self.n_steps,\n",
      "            self.observation_space,  # type: ignore[arg-type]\n",
      "            self.action_space,\n",
      "            device=self.device,\n",
      "            gamma=self.gamma,\n",
      "            gae_lambda=self.gae_lambda,\n",
      "            n_envs=self.n_envs,\n",
      "            **self.rollout_buffer_kwargs,\n",
      "        )\n",
      "        self.policy = self.policy_class(  # type: ignore[assignment]\n",
      "            self.observation_space, self.action_space, self.lr_schedule, use_sde=self.use_sde, **self.policy_kwargs\n",
      "        )\n",
      "        self.policy = self.policy.to(self.device)\n",
      "\n",
      "    def collect_rollouts(\n",
      "        self,\n",
      "        env: VecEnv,\n",
      "        callback: BaseCallback,\n",
      "        rollout_buffer: RolloutBuffer,\n",
      "        n_rollout_steps: int,\n",
      "    ) -> bool:\n",
      "        \"\"\"\n",
      "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
      "        The term rollout here refers to the model-free notion and should not\n",
      "        be used with the concept of rollout used in model-based RL or planning.\n",
      "\n",
      "        :param env: The training environment\n",
      "        :param callback: Callback that will be called at each step\n",
      "            (and at the beginning and end of the rollout)\n",
      "        :param rollout_buffer: Buffer to fill with rollouts\n",
      "        :param n_rollout_steps: Number of experiences to collect per environment\n",
      "        :return: True if function returned with at least `n_rollout_steps`\n",
      "            collected, False if callback terminated rollout prematurely.\n",
      "        \"\"\"\n",
      "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
      "        # Switch to eval mode (this affects batch norm / dropout)\n",
      "        self.policy.set_training_mode(False)\n",
      "\n",
      "        n_steps = 0\n",
      "        rollout_buffer.reset()\n",
      "        # Sample new weights for the state dependent exploration\n",
      "        if self.use_sde:\n",
      "            self.policy.reset_noise(env.num_envs)\n",
      "\n",
      "        callback.on_rollout_start()\n",
      "\n",
      "        while n_steps < n_rollout_steps:\n",
      "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
      "                # Sample a new noise matrix\n",
      "                self.policy.reset_noise(env.num_envs)\n",
      "\n",
      "            with th.no_grad():\n",
      "                # Convert to pytorch tensor or to TensorDict\n",
      "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
      "                actions, values, log_probs = self.policy(obs_tensor)\n",
      "            actions = actions.cpu().numpy()\n",
      "\n",
      "            # Rescale and perform action\n",
      "            clipped_actions = actions\n",
      "\n",
      "            if isinstance(self.action_space, spaces.Box):\n",
      "                if self.policy.squash_output:\n",
      "                    # Unscale the actions to match env bounds\n",
      "                    # if they were previously squashed (scaled in [-1, 1])\n",
      "                    clipped_actions = self.policy.unscale_action(clipped_actions)\n",
      "                else:\n",
      "                    # Otherwise, clip the actions to avoid out of bound error\n",
      "                    # as we are sampling from an unbounded Gaussian distribution\n",
      "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
      "\n",
      "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
      "\n",
      "            self.num_timesteps += env.num_envs\n",
      "\n",
      "            # Give access to local variables\n",
      "            callback.update_locals(locals())\n",
      "            if not callback.on_step():\n",
      "                return False\n",
      "\n",
      "            self._update_info_buffer(infos)\n",
      "            n_steps += 1\n",
      "\n",
      "            if isinstance(self.action_space, spaces.Discrete):\n",
      "                # Reshape in case of discrete action\n",
      "                actions = actions.reshape(-1, 1)\n",
      "\n",
      "            # Handle timeout by bootstraping with value function\n",
      "            # see GitHub issue #633\n",
      "            for idx, done in enumerate(dones):\n",
      "                if (\n",
      "                    done\n",
      "                    and infos[idx].get(\"terminal_observation\") is not None\n",
      "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
      "                ):\n",
      "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
      "                    with th.no_grad():\n",
      "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
      "                    rewards[idx] += self.gamma * terminal_value\n",
      "\n",
      "            rollout_buffer.add(\n",
      "                self._last_obs,  # type: ignore[arg-type]\n",
      "                actions,\n",
      "                rewards,\n",
      "                self._last_episode_starts,  # type: ignore[arg-type]\n",
      "                values,\n",
      "                log_probs,\n",
      "            )\n",
      "            self._last_obs = new_obs  # type: ignore[assignment]\n",
      "            self._last_episode_starts = dones\n",
      "\n",
      "        with th.no_grad(): #10th bug: 1\n",
      "            # Compute value for the last timestep\n",
      "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))  # type: ignore[arg-type]\n",
      "\n",
      "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
      "\n",
      "        callback.update_locals(locals())\n",
      "\n",
      "        callback.on_rollout_end()\n",
      "\n",
      "        return True\n",
      "\n",
      "    def train(self) -> None:\n",
      "        \"\"\"\n",
      "        Consume current rollout data and update policy parameters.\n",
      "        Implemented by individual algorithms.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def learn(\n",
      "        self: SelfOnPolicyAlgorithm,\n",
      "        total_timesteps: int,\n",
      "        callback: MaybeCallback = None,\n",
      "        log_interval: int = 1,\n",
      "        tb_log_name: str = \"OnPolicyAlgorithm\",\n",
      "        reset_num_timesteps: bool = True,\n",
      "        progress_bar: bool = False,\n",
      "    ) -> SelfOnPolicyAlgorithm:\n",
      "        iteration = 0\n",
      "\n",
      "        total_timesteps, callback = self._setup_learn(\n",
      "            total_timesteps,\n",
      "            callback,\n",
      "            reset_num_timesteps,\n",
      "            tb_log_name,\n",
      "            progress_bar,\n",
      "        )\n",
      "\n",
      "        callback.on_training_start(locals(), globals())\n",
      "\n",
      "        assert self.env is not None\n",
      "\n",
      "        while self.num_timesteps < total_timesteps:\n",
      "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "\n",
      "            if not continue_training:\n",
      "                break\n",
      "\n",
      "            iteration += 1\n",
      "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
      "\n",
      "            # Display training infos\n",
      "            if log_interval is not None and iteration % log_interval == 0:\n",
      "                assert self.ep_info_buffer is not None\n",
      "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
      "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
      "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
      "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
      "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
      "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
      "                self.logger.record(\"time/fps\", fps)\n",
      "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
      "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
      "                self.logger.dump(step=self.num_timesteps)\n",
      "\n",
      "            self.train()\n",
      "\n",
      "        callback.on_training_end()\n",
      "\n",
      "        return self\n",
      "\n",
      "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
      "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
      "\n",
      "        return state_dicts, []\n",
      "\n",
      "round: 0----\n",
      "creating new model\n",
      "round: 1----\n",
      "creating new model\n",
      "round: 2----\n",
      "creating new model\n",
      "round: 3----\n",
      "creating new model\n",
      "round: 4----\n",
      "creating new model\n",
      "round: 5----\n",
      "creating new model\n",
      "round: 6----\n",
      "creating new model\n",
      "round: 7----\n",
      "creating new model\n",
      "round: 8----\n",
      "creating new model\n",
      "round: 9----\n",
      "creating new model\n",
      "round: 10----\n",
      "creating new model\n",
      "round: 11----\n",
      "creating new model\n",
      "round: 12----\n",
      "creating new model\n",
      "round: 13----\n",
      "creating new model\n",
      "round: 14----\n",
      "creating new model\n",
      "round: 15----\n",
      "creating new model\n",
      "round: 16----\n",
      "creating new model\n",
      "round: 17----\n",
      "creating new model\n",
      "round: 18----\n",
      "creating new model\n",
      "round: 19----\n",
      "creating new model\n",
      "round: 20----\n",
      "creating new model\n",
      "round: 21----\n",
      "creating new model\n",
      "round: 22----\n",
      "creating new model\n",
      "round: 23----\n",
      "creating new model\n",
      "round: 24----\n",
      "creating new model\n"
     ]
    }
   ],
   "source": [
    "import frozenlake.testing_SB3_Frozenlake as Frozenlake_Experiment\n",
    "\n",
    "# initialize bug_version_list\n",
    "bug_version_list = [\n",
    "    [],\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [6],\n",
    "    # [7],\n",
    "    # [8],\n",
    "    [9],\n",
    "    [10],\n",
    "    # [11],\n",
    "    # [12],\n",
    "    # [13],\n",
    "    # [14],\n",
    "    # [15],\n",
    "]\n",
    "\n",
    "Frozenlake_Experiment.main(bug_version_list=bug_version_list, rounds=25, epochs=300, model_type='dqn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mountaincar Testbed (continous) Experiment\n",
    "\n",
    "- Find log files in RLTestig/logs/mountaincar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountaincar.testing_SB3_Mountaincar as Mountaincar_Experiment\n",
    "\n",
    "bug_version_list = [\n",
    "    [],\n",
    "    # [5],\n",
    "    # [6],\n",
    "    # [7],\n",
    "    # [8],\n",
    "    # [10],\n",
    "    # [11],\n",
    "    # [12],\n",
    "    # [13],\n",
    "    # [14],\n",
    "    # [15]\n",
    "]\n",
    "\n",
    "Mountaincar_Experiment.main(bug_version_list=bug_version_list, rounds=25, epochs=300, model_type='ppo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import log_parser\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import config_parser\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "\n",
    "root_dir = config_parser.parserConfig()['root_dir']\n",
    "\n",
    "\n",
    "def linelar_regression(data, title = 'Data Trend Analysis', show=False):\n",
    "\n",
    "    # 将数据转换成Pandas Series对象\n",
    "    series = pd.Series(data)\n",
    "\n",
    "    # 计算简单移动平均(SMA)和指数移动平均(EMA)\n",
    "    sma = series.rolling(window=3).mean()\n",
    "    ema = series.ewm(span=3, adjust=False).mean()\n",
    "\n",
    "    # 使用线性回归判断趋势\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(range(len(data)), data)\n",
    "    \n",
    "    if show:\n",
    "        # 绘制数据和趋势线\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data, label='Original Data', marker='o')\n",
    "        plt.plot(range(len(data)), intercept + slope * np.asarray(range(len(data))), 'r', label=f'Trend Line: slope={slope:.2f}')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    # 输出线性回归结果\n",
    "    # print(f\"Slope of trend line: {slope:.2f}\")\n",
    "\n",
    "    # 如果斜率显著小于0，我们可以认为不存在上升趋势\n",
    "    if slope < 0:\n",
    "        return 'false'\n",
    "    else:\n",
    "        return 'true'\n",
    "    \n",
    "\n",
    "def test_true_proportion(num_samples, num_true, sig_level=0.05, expected_prob=0.95):\n",
    "    \"\"\"\n",
    "    对观察到的 'true' 样本数进行二项检验。\n",
    "\n",
    "    :param num_samples: 样本总数\n",
    "    :param num_true: 观察到的 'true' 样本数\n",
    "    :param sig_level: 显著性水平（默认为 0.05）\n",
    "    :param expected_prob: 预期成功概率（默认为 0.5）\n",
    "    :return: p 值和是否显著\n",
    "    \"\"\"\n",
    "    # 计算二项检验的 p 值\n",
    "    p_value = binomtest(k=num_true, n=num_samples, p=expected_prob, alternative='greater')\n",
    "\n",
    "    # 判断是否显著\n",
    "    is_significant = p_value < sig_level\n",
    "\n",
    "    return p_value, is_significant\n",
    "\n",
    "\n",
    "def bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=False):\n",
    "    # 使用 Path.rglob() 获取所有子文件\n",
    "    # '**/*' 表示匹配所有的文件和文件夹, 如果只想匹配文件, 使用 '**/*.*'\n",
    "    bug_free_log_list = [file for file in bug_free_log_path.rglob('*') if file.is_file()]\n",
    "    print(bug_free_log_list)\n",
    "\n",
    "    result = []\n",
    "    for path in bug_free_log_list:\n",
    "        # print(log_parser.parse_log_file(path))\n",
    "        data = log_parser.parse_log_file_fuzzy(path)\n",
    "        if len(data)> 0:\n",
    "            temp = linelar_regression(data, title=path, show=show_fig)\n",
    "        result.append(temp)\n",
    "\n",
    "    p_value = binomtest(result.count('true'), len(result), alternative='greater')\n",
    "    print(p_value)\n",
    "\n",
    "    \n",
    "    \n",
    "def bin_test_mountaincar(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Mountaincar', 'sac', '[]')), show_fig=False):\n",
    "    # 使用 Path.rglob() 获取所有子文件\n",
    "    # '**/*' 表示匹配所有的文件和文件夹, 如果只想匹配文件, 使用 '**/*.*'\n",
    "    bug_free_log_list = [file for file in bug_free_log_path.rglob('*') if file.is_file()]\n",
    "\n",
    "    print(bug_free_log_list)\n",
    "\n",
    "    # accuracy_list = []\n",
    "    # 打印所有文件路径\n",
    "    result = []\n",
    "    for path in bug_free_log_list:\n",
    "        data = log_parser.parse_mountaincar_log_file(path)\n",
    "        if len(data) > 0:\n",
    "            temp = linelar_regression(data, title=path, show=show_fig)\n",
    "        result.append(temp)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    在进行二项检验时，零假设（null hypothesis）通常是观察到的成功次数与随机变化相一致，即成功的概率等于 expected_prob。如果实际观察到的成功次数显著高于（或低于）这个期望成功概率，那么您可能会得到一个很小的 p 值，从而拒绝零假设。\n",
    "    '''\n",
    "    p_value = binomtest(result.count('true'), len(result), alternative='greater')\n",
    "    print(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_0'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_1'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_10'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_11'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_12'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_13'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_14'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_15'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_16'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_17'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_18'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_19'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_2'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_20'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_21'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_22'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_23'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_24'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_3'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_4'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_5'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_6'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_7'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_8'), WindowsPath('C:/Users/iansy/Documents/GitHub/stable-baselines3-testing-new/RLTesting/logs/Frozenlake/dqn/[]/time_2024-03-02[]round_9')]\n",
      "BinomTestResult(k=24, n=25, alternative='greater', statistic=0.96, pvalue=7.748603820800781e-07)\n"
     ]
    }
   ],
   "source": [
    "# bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=False)\n",
    "# bin_test_mountaincar(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Mountaincar', 'sac', '[]')), show_fig=False)\n",
    "bin_test_frozenlake(bug_free_log_path = Path(os.path.join(root_dir, 'RLTesting', 'logs', 'Frozenlake', 'dqn', '[]')), show_fig=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB3Testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
